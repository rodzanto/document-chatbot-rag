{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Bedrock and Kendra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "* You must configure your Amazon Kendra index before running this notebook. For instructions check the documentation here: https://docs.aws.amazon.com/kendra/latest/dg/create-index.html\n",
    "* During the preview, your account must have access to Amazon Bedrock, or you should have a shared role or profile with access to it\n",
    "* During the preview, you must download the Bedrock SDK for installing in the next cells\n",
    "\n",
    "For more information on Bedrock topics check the user guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.0.200)\n",
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/59/d0/074f7fbd7323623cca4175e0323c2cff565d5cf8c6b58f5dc81f046aa29f/langchain-0.0.240-py3-none-any.whl.metadata\n",
      "  Using cached langchain-0.0.240-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.4.49)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.14)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.30.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Using cached langchain-0.0.240-py3-none-any.whl (1.4 MB)\n",
      "Installing collected packages: langchain\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.200\n",
      "    Uninstalling langchain-0.0.200:\n",
      "      Successfully uninstalled langchain-0.0.200\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aws-langchain 0.0.1 requires langchain==0.0.137, but you have langchain 0.0.240 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-0.0.240\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ../dependencies/boto3-1.26.165-py3-none-any.whl --quiet\n",
    "%pip install ../dependencies/botocore-1.29.165-py3-none-any.whl --quiet\n",
    "%pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Un comment the following lines to run from your local environment outside of the AWS account with Bedrock access\n",
    "\n",
    "#import os\n",
    "#os.environ['BEDROCK_ASSUME_ROLE'] = 'arn:aws:iam::191767470724:role/PowerUserRole'\n",
    "#os.environ['AWS_PROFILE'] = 'bedrock-internal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
    "#boto3_bedrock = bedrock.get_bedrock_client(os.environ.get('BEDROCK_ASSUME_ROLE', None))\n",
    "\n",
    "bedrock = boto3.client(\n",
    " service_name='bedrock',\n",
    " region_name='us-east-1',\n",
    " endpoint_url='https://bedrock.us-east-1.amazonaws.com'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will be using the Titan Embeddings Model to generate our Embeddings.\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# - create the Anthropic Model\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\", client=bedrock, model_kwargs={'max_tokens_to_sample':8000})\n",
    "bedrock_embeddings = BedrockEmbeddings(client=bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the context, Amazon Kendra learns query suggestions based on queries that users search\n",
      "for. Specifically:\n",
      "\n",
      "- It continuously analyzes the queries in a customer's query log\n",
      "- It identifies frequently searched queries and popular queries\n",
      "- It suggests these popular queries to users as they start typing\n",
      "- It requires a minimum number of unique users (default 3) to have searched a query before\n",
      "suggesting it\n",
      "- It evaluates how recently the queries were searched based on a \"query log time window\" set by the\n",
      "customer\n",
      "- It can either learn from all queries or only queries that include user information, depending on a\n",
      "customer configuration\n",
      "\n",
      "So in summary, Amazon Kendra learns by analyzing a customer's query logs and identifying frequently\n",
      "searched queries to then suggest to users.\n",
      "\n",
      "Source(s):\n",
      "https://docs.aws.amazon.com/kendra/latest/APIReference/API_DescribeQuerySuggestionsConfig.html\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import AmazonKendraRetriever\n",
    "import boto3\n",
    "\n",
    "kendra_index_id = '2c1575af-b7aa-44cb-ae01-454598936576'\n",
    "region = 'us-east-1'\n",
    "\n",
    "kendra_client = boto3.client('kendra')\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Assistant:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=AmazonKendraRetriever(\n",
    "        index_id=kendra_index_id,\n",
    "        region_name=region,\n",
    "        client=kendra_client,\n",
    "        attribute_filter={\n",
    "            'EqualsTo': {\n",
    "                'Key': '_language_code',\n",
    "                'Value': {'StringValue': 'es'}\n",
    "            }\n",
    "        }\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")\n",
    "query = \"How does Amazon Kendra learn?\"\n",
    "result = qa({\"query\": query})\n",
    "print_ww(f\"{result['result']}\\n\\nSource(s):\\n{result['source_documents'][0].metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Streamlit UI for this RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit_chat import message\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import Dict\n",
    "import json\n",
    "from io import StringIO\n",
    "from random import randint\n",
    "import boto3\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "st.set_page_config(page_title=\"Retrieval Augmented Generation\", page_icon=\":robot:\", layout=\"wide\")\n",
    "st.header(\"Document Insights Chatbot with Amazon Bedrock\")\n",
    "\n",
    "bedrock = boto3.client(\n",
    " service_name='bedrock',\n",
    " region_name='us-east-1',\n",
    " endpoint_url='https://bedrock.us-east-1.amazonaws.com'\n",
    ")\n",
    "\n",
    "# KENDRA ------------\n",
    "\n",
    "from langchain.retrievers import AmazonKendraRetriever\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "kendra_index_id = '2c1575af-b7aa-44cb-ae01-454598936576'\n",
    "region = 'us-east-1'\n",
    "\n",
    "kendra_client = boto3.client('kendra')\n",
    "\n",
    "# - create the Anthropic Model\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\", client=bedrock, model_kwargs={'max_tokens_to_sample':8000})\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "vectorstore_pinecone = Pinecone.from_existing_index(\n",
    "    embedding=bedrock_embeddings,\n",
    "    index_name = index_name\n",
    ")\n",
    "\n",
    "wrapper_store_pinecone = VectorStoreIndexWrapper(vectorstore=vectorstore_pinecone)\n",
    "\n",
    "\n",
    "# LANGCHAIN ------------\n",
    "\n",
    "#qa = RetrievalQA.from_chain_type(\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Human: Consider the context in the <context></context> XML tags and your own knowledge, to answer the question at the end.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=AmazonKendraRetriever(\n",
    "        index_id=kendra_index_id,\n",
    "        region_name=region,\n",
    "        client=kendra_client,\n",
    "        #attribute_filter={\n",
    "        #    'EqualsTo': {\n",
    "        #        'Key': '_language_code',\n",
    "        #        'Value': {'StringValue': 'es'}\n",
    "        #    }\n",
    "        #}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#@st.cache_resource\n",
    "#def load_chain(_prompt):\n",
    "chatchain = RetrievalQA.from_chain_type(\n",
    "        llm = Bedrock(\n",
    "            #model_id ='anthropic.claude-instant-v1'\n",
    "            model_id='anthropic.claude-v1',\n",
    "            client=bedrock,\n",
    "            model_kwargs={\n",
    "                'max_tokens_to_sample':8000,\n",
    "                'temperature':0,\n",
    "                'top_p':0.9,\n",
    "                'stop_sequences': [\"Human\"]\n",
    "            }\n",
    "        ),\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=AmazonKendraRetriever(\n",
    "            index_id=kendra_index_id,\n",
    "            region_name=region,\n",
    "            client=kendra_client,\n",
    "            #attribute_filter={\n",
    "            #    'EqualsTo': {\n",
    "            #        'Key': '_language_code',\n",
    "            #        'Value': {'StringValue': 'es'}\n",
    "            #    }\n",
    "            #}\n",
    "        ),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "memory = ConversationBufferMemory()\n",
    "chain = ConversationChain(llm=llm, memory=memory)\n",
    "#    return chain\n",
    "\n",
    "#chatchain = load_chain(prompt)\n",
    "\n",
    "# initialise session variables\n",
    "if 'generated' not in st.session_state:\n",
    "    st.session_state['generated'] = []\n",
    "if 'past' not in st.session_state:\n",
    "    st.session_state['past'] = []\n",
    "if 'widget_key' not in st.session_state:\n",
    "    st.session_state['widget_key'] = str(randint(1000, 100000000))\n",
    "\n",
    "# Sidebar - the clear button is will flush the memory of the conversation\n",
    "#st.sidebar.title(\"Sidebar\")\n",
    "st.sidebar.image('./images/AWS_logo_RGB.png', width=150)\n",
    "\n",
    "st.markdown(\n",
    "    f'''\n",
    "        <style>\n",
    "            .sidebar .sidebar-content {{\n",
    "                width: 150px;\n",
    "            }}\n",
    "        </style>\n",
    "    ''',\n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "\n",
    "# this is the container that displays the past conversation\n",
    "response_container = st.container()\n",
    "# this is the container with the input text box\n",
    "container = st.container()\n",
    "\n",
    "with container:\n",
    "    # define the input text box\n",
    "    with st.form(key='my_form', clear_on_submit=True):\n",
    "        user_input = st.text_area(\"You:\", key='input', height=50)\n",
    "        submit_button = st.form_submit_button(label='Send')\n",
    "\n",
    "    # when the submit button is pressed we send the user query to the chatchain object and save the chat history\n",
    "    if submit_button and user_input:\n",
    "        #input_prompt = prompt.format(\n",
    "        #    user_input=user_input,\n",
    "        #)\n",
    "        output = chatchain({\"query\": user_input})\n",
    "        for i in output[\"source_documents\"]:\n",
    "            sources = i.metadata[\"source\"]\n",
    "        #output = chatchain(input_prompt)[\"response\"]\n",
    "        st.session_state['past'].append(user_input)\n",
    "        if sources:\n",
    "            st.session_state['generated'].append(output[\"result\"] + \"Source document(s):\\n\" + sources)\n",
    "            sources = \"\"\n",
    "        else:\n",
    "            st.session_state['generated'].append(output[\"result\"])\n",
    "\n",
    "# this loop is responsible for displaying the chat history\n",
    "if st.session_state['generated']:\n",
    "    with response_container:\n",
    "        for i in range(len(st.session_state['generated'])):\n",
    "            message(st.session_state[\"past\"][i], is_user=True, key=str(i) + '_user', avatar_style=\"adventurer\", seed=120)\n",
    "            message(st.session_state[\"generated\"][i], key=str(i), avatar_style=\"bottts\", seed=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
